{"author": "lrytz", "commit_date": "2021/09/09 19:29:55", "commit_message": "Update scala-parallel-collections to 1.0.3\n\nThe changes are actually less than the version bump might suggest.\n  - removed OSGi metadata\n  - renamed some internal inner classes\n  - added `Automatic-Module-Name`", "title": "[SPARK-36712][Build] Make scala-parallel-collections in 2.13 POM a direct dependency (not in maven profile)", "body": "As [reported on `dev@spark.apache.org`](https://lists.apache.org/thread.html/r84cff66217de438f1389899e6d6891b573780159cd45463acf3657aa%40%3Cdev.spark.apache.org%3E), the published POMs when building with Scala 2.13 have the `scala-parallel-collections` dependency only in the `scala-2.13` profile of the pom.\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis PR suggests to work around this by un-commenting the `scala-parallel-collections` dependency when switching to 2.13 using the the `change-scala-version.sh` script.\r\n\r\nI included an upgrade to scala-parallel-collections version 1.0.3, the changes compared to 0.2.0 are minor.\r\n  - removed OSGi metadata\r\n  - renamed some internal inner classes\r\n  - added `Automatic-Module-Name`\r\n\r\n### Why are the changes needed?\r\n\r\nAccording to the posts, this solves issues for developers that write unit tests for their applications.\r\n\r\nStephen Coy suggested to use the https://www.mojohaus.org/flatten-maven-plugin. While this sounds like a more principled solution, it is possibly too risky to do at this specific point in time?\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nLocally", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "6", "deletions": "9", "changes": "15"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/change-scala-version.sh", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "external/avro/pom.xml", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10-sql/pom.xml", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/pom.xml", "additions": "6", "deletions": "12", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/pom.xml", "additions": "6", "deletions": "12", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "9", "changes": "16"}, "updated": [1, 4, 22]}, {"file": {"name": "sql/catalyst/pom.xml", "additions": "6", "deletions": "12", "changes": "18"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/pom.xml", "additions": "6", "deletions": "9", "changes": "15"}, "updated": [2, 2, 5]}, {"file": {"name": "sql/hive-thriftserver/pom.xml", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/pom.xml", "additions": "6", "deletions": "9", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/pom.xml", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/08/09 23:24:04", "commit_message": "Update test results.", "title": "[SPARK-36461][SQL] Enable ObjectHashAggregate for more Aggregate functions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nEnabing more `ObjectHashAggregate` for more aggregate functions, as it has better performance compared to `SortAggregate` according to current benchmark.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nWe are pretty conservative with `ObjectHashAggregate` right now. Except for enabling it by configuring, there is also a fallback threshold. With the fallback threshold, it looks like that even `ObjectHashAggregate` is bad at memory estimation, it is unlikely to OOM easily. For now the users cannot work with `ObjectHashAggregate` for these aggregation functions even they enable the config. That's said, `ObjectHashAggregate` is not much able to be applicable under these limitations. As we are enough conservative with fallback threshold value, it is also disable-able using the config, the limitations on applicable aggregation functions look not necessary.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. Some aggregations will be applied with `ObjectHashAggregate` after this change.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nAdded unit tests.", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "viirya", "commit_date": "2021/06/24 16:20:00", "commit_message": "Enable ObjectHashAggregate for more cases.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "9", "deletions": "19", "changes": "28"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "8", "deletions": "18", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala", "additions": "41", "deletions": "4", "changes": "45"}, "updated": [0, 0, 0]}]}{"author": "viirya", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}{"author": "viirya", "commit_date": "2020/08/01 16:49:47", "commit_message": "Upgrade guava and hadoop.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.IntervalExpressionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 3, 5]}, {"file": {"name": "pom.xml", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 5, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/09 23:59:43", "commit_message": "isin", "title": "[SPARK-36618][PYTHON] Support dropping rows of a single-indexed DataFrame", "body": "### What changes were proposed in this pull request?\r\nSupport dropping rows of a single-indexed DataFrame.\r\n\r\nDropping rows and columns at the same time is supported in this PR  as well.\r\n\r\n\r\n### Why are the changes needed?\r\nTo increase pandas API coverage.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, dropping rows of a single-indexed DataFrame is supported now.\r\n\r\n```py\r\n>>> df = ps.DataFrame(np.arange(12).reshape(3, 4), columns=['A', 'B', 'C', 'D'])\r\n>>> df\r\n   A  B   C   D\r\n0  0  1   2   3\r\n1  4  5   6   7\r\n2  8  9  10  11\r\n```\r\n#### From\r\n```py\r\n>>> df.drop([0, 1])\r\nTraceback (most recent call last):\r\n...\r\nKeyError: [(0,), (1,)]\r\n\r\n>>> df.drop([0, 1], axis=0)\r\nTraceback (most recent call last):\r\n...\r\nNotImplementedError: Drop currently only works for axis=1\r\n\r\n>>> df.drop(1)\r\nTraceback (most recent call last):\r\n...\r\nKeyError: [(1,)]\r\n\r\n>>> df.drop(index=1)\r\nTraceback (most recent call last):\r\n...\r\nTypeError: drop() got an unexpected keyword argument 'index'\r\n\r\n>>> df.drop(index=[0, 1], columns='A')\r\nTraceback (most recent call last):\r\n...\r\nTypeError: drop() got an unexpected keyword argument 'index'\r\n```\r\n#### To\r\n```py\r\n>>> df.drop([0, 1])\r\n   A  B   C   D\r\n2  8  9  10  11\r\n\r\n>>> df.drop([0, 1], axis=0)\r\n   A  B   C   D\r\n2  8  9  10  11\r\n\r\n>>> df.drop(1)\r\n   A  B   C   D\r\n0  0  1   2   3\r\n2  8  9  10  11\r\n\r\n>>> df.drop(index=1)\r\n   A  B   C   D\r\n0  0  1   2   3\r\n2  8  9  10  11\r\n\r\n>>> df.drop(index=[0, 1], columns='A')\r\n   B   C   D\r\n2  9  10  11\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [1, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [1, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 1, 1]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/01 18:59:43", "commit_message": "refactor", "title": "[SPARK-36628][PYTHON] Implement `__getitem__`  of label-based MultiIndex select", "body": "### What changes were proposed in this pull request?\r\nImplement `__getitem__`  of label-based MultiIndex select\r\n\r\n\r\n### Why are the changes needed?\r\nIncrease pandas API coverage\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, reading values by label-based MultiIndex select is supported now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...             {\"a\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"b\": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\r\n...             index=[0, 1, 3, 5, 6, 8, 9, 9, 9],\r\n...         )\r\n>>> psdf = psdf.set_index(\"b\", append=True)\r\n>>> psdf\r\n     a\r\n  b   \r\n0 4  1\r\n1 5  2\r\n3 6  3\r\n5 3  4\r\n6 2  5\r\n8 1  6\r\n9 0  7\r\n  0  8\r\n  0  9\r\n>>> psdf.loc[6, 2]\r\n     a\r\n  b   \r\n6 2  5\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/10 21:37:08", "commit_message": "empty", "title": "[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode", "body": "### What changes were proposed in this pull request?\r\nImplement DataFrame.mode (along index axis).\r\n\r\n\r\n### Why are the changes needed?\r\nGet the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. `DataFrame.mode` can be used now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...     [(\"bird\", 2, 2), (\"mammal\", 4, np.nan), (\"arthropod\", 8, 0), (\"bird\", 2, np.nan)],\r\n...     index=(\"falcon\", \"horse\", \"spider\", \"ostrich\"),\r\n...     columns=(\"species\", \"legs\", \"wings\"),\r\n... )\r\n>>> psdf\r\n           species  legs  wings                                                 \r\nfalcon        bird     2    2.0\r\nhorse       mammal     4    NaN\r\nspider   arthropod     8    0.0\r\nostrich       bird     2    NaN\r\n\r\n>>> psdf.mode()\r\n  species  legs  wings\r\n0    bird   2.0    0.0\r\n1    None   NaN    2.0\r\n\r\n>>> psdf.mode(dropna=False)\r\n  species  legs  wings\r\n0    bird     2    NaN\r\n\r\n>>> psdf.mode(numeric_only=True)\r\n   legs  wings\r\n0   2.0    0.0\r\n1   NaN    2.0\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [0, 5, 20]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "xinrong-databricks", "commit_date": "2021/09/07 21:11:48", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexing", "pyspark.pandas.tests.test_groupby"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "91", "deletions": "58", "changes": "149"}, "updated": [0, 3, 11]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "62", "deletions": "12", "changes": "74"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}{"author": "xinrong-databricks", "commit_date": "2021/08/03 22:40:45", "commit_message": "DataFrame.mode", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [1, 5, 21]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 2, 6]}]}{"author": "karenfeng", "commit_date": "2021/08/04 17:22:45", "commit_message": "Delimit table with comments\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "[SPARK-36405] Check that SQLSTATEs are valid", "body": "### What changes were proposed in this pull request?\r\n\r\nAdds validation that the SQLSTATEs in the error class JSON are a subset of those provided in the README.\r\n\r\n### Why are the changes needed?\r\n\r\nValidation of error class JSON\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nUnit test", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "karenfeng", "commit_date": "2021/08/04 03:05:32", "commit_message": "Check that SQLSTATEs are valid\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 1]}]}{"author": "planga82", "commit_date": "2021/09/09 00:57:53", "commit_message": "Apply only to unresolved regex", "title": "[SPARK-36698][SQL] Allow expand 'quotedRegexColumnNames' in all expressions when it\u2019s expanded to one named expression", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWith `spark.sql.parser.quotedRegexColumnNames=true` regular expressions are not allowed in expressions like  \r\n``` SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) ``` \r\n\r\nThis PR propose to improve this behavior and allow this regular expression when it expands to only one named expression. It\u2019s the same behavior in Hive.\r\n\r\nExample 1:\r\n```\r\nSELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_.* expands to col_a:  OK\r\n\r\nExample 2:\r\n```\r\nSELECT `col_.*`/col_b FROM (SELECT 3 AS col_a, 1 as col_b) \r\n```\r\ncol_.* expands to (col_a, col_b) : Fail like now\r\n\r\nExample 3:\r\n```\r\nSELECT `col_a`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_a expands to col_a:  OK\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove this feature and approaching hive behavior\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/08/15 22:47:03", "commit_message": "Fix style", "title": "[SPARK-36453][SQL] Improve consistency processing floating point special literals", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSpecial literals in floating point are not consistent between cast and json expressions\r\n```\r\nscala> spark.sql(\"SELECT CAST('+Inf' as Double)\").show\r\n+--------------------+                                                        \r\n|CAST(+Inf AS DOUBLE)|\r\n+--------------------+\r\n|            Infinity|\r\n+--------------------+\r\n```\r\n```\r\nscala> val schema =  StructType(StructField(\"a\", DoubleType) :: Nil)\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col1\").select(from_json(col(\"col1\"),schema)).show\r\n+---------------+\r\n|from_json(col1)|\r\n+---------------+\r\n|         {null}|\r\n+---------------+\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col\").withColumn(\"col\", from_json(col(\"col\"), StructType.fromDDL(\"a DOUBLE\"))).write.json(\"/tmp/jsontests12345\")\r\nscala> spark.read.schema(StructType(Seq(StructField(\"col\",schema)))).json(\"/tmp/jsontests12345\").show\r\n+------+\r\n|   col|\r\n+------+\r\n|{null}|\r\n+------+\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove consistency between operations\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, we are going to support the same special literal in Cast and Json expressions\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing and manual testing", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 4, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "planga82", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}{"author": "planga82", "commit_date": "2021/08/08 22:45:39", "commit_message": "improvement + tests", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2021/09/02 18:25:03", "commit_message": "Add Min Shen as lead author", "title": "[WIP][SPARK-33701][SHUFFLE] Adaptive shuffle merge finalization for push-based shuffle", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAs part of SPARK-32920 implemented a simple approach to finalization for push-based shuffle. Shuffle merge finalization is the final operation happens at the end of the stage when all the tasks are completed asking all the external shuffle services to complete the shuffle merge for the stage. Once this request is completed no more shuffle pushes will be accepted. With this approach, `DAGScheduler` waits for a fixed time of 10s (`spark.shuffle.push.finalize.timeout`) to allow some time for the inflight shuffle pushes to complete, but this adds additional overhead to stages with very little shuffles.\r\n\r\nIn this PR, instead of waiting for fixed amount of time before shuffle merge finalization now this is controlled adaptively if min threshold number of map tasks shuffle push (`spark.shuffle.push.minPushRatio`) completed then shuffle merge finalization will be scheduled. Also additionally if the total shuffle generated is lesser than min threshold shuffle size (`spark.shuffle.push.minShuffleSizeToWait`) then immediately shuffle merge finalization is scheduled.\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThis is a performance improvement to the existing functionality\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes additional user facing configs `spark.shuffle.push.minPushRatio` and `spark.shuffle.push.minShuffleSizeToWait`\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit tests in `DAGSchedulerSuite`\r\n\r\nLead-authored-by: Min Shen <mshen@linkedin.com>\r\nCo-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "venkata91", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}{"author": "venkata91", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/08/13 14:26:41", "commit_message": "fix", "title": "[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nUse short string as executor pod name prefix if app name is long.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf we have a long spark app name and start with k8s master, we will get the execption.\r\n```\r\njava.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)\r\n\tat org.apache.spark.SparkConf.get(SparkConf.scala:261)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)\r\n\tat org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)\r\n```\r\nUse app name as the executor pod name is the Spark internal behavior and we should not make application failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test\r\n\r\nthe new log:\r\n```\r\n21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.\r\n21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926\r\n21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.\r\n21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\r\n21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\r\n```\r\n\r\nverify the config:\r\n![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)\r\n\r\nverify the executor pod name\r\n![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "ulysses-you", "commit_date": "2021/06/08 05:56:59", "commit_message": "Support optimize skew join even if introduce extra shuffle", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "17", "deletions": "14", "changes": "31"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "19", "deletions": "6", "changes": "25"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/simpleCosting.scala", "additions": "39", "deletions": "5", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "59", "deletions": "29", "changes": "88"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 5, 8]}]}{"author": "ulysses-you", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "ulysses-you", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}{"author": "pralabhkumar", "commit_date": "2021/09/07 13:02:45", "commit_message": "Addressed review comments", "title": "[SPARK-36622][CORE] Making spark.history.kerberos.principal _HOST compliant", "body": "### What changes were proposed in this pull request?\r\nspark.history.kerberos.principal can have _HOST , which will be replaced by host canonical address\r\n\r\n### Why are the changes needed?\r\nThis change is required for user to provide prinicipal _HOST complaint . User don't need to hardcode the History server URL . This is in line with Hiveserver2, livy server and other hadoop components.  \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, users can now add _HOST in the spark.history.kerberos.principal\r\n\r\n### How was this patch tested?\r\n\r\nunit tests/local testing\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkHadoopUtilSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/09/08 09:26:29", "commit_message": "trigger test", "title": "[SPARK-35437][SQL] Use expressions to filter Hive partitions at client side", "body": "### What changes were proposed in this pull request?\r\nImprove partition filtering speed and reduce metastore pressure.\r\nWe can first pull all the partition names, filter by expressions, and then obtain detailed information about the corresponding partitions from the MetaStore Server.\r\n\r\n### Why are the changes needed?\r\nWhen `convertFilters` cannot take effect, cannot filter the queried partitions in advance on the hive MetaStore Server. At this time, `getAllPartitionsOf` will get all partition details.\r\n\r\nWhen the Hive client cannot use the server filter, it will first obtain the values of all partitions, and then filter.\r\n\r\nWhen we have a table with a lot of partitions and there is no way to filter it on the MetaStore Server, we will get all the partition details and filter it on the client side. This is slow and puts a lot of pressure on the MetaStore Server.\r\n\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}{"author": "cxzl25", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}{"author": "cxzl25", "commit_date": "2021/06/28 04:56:59", "commit_message": "Create hive permanent function with owner name", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}{"author": "cxzl25", "commit_date": "2021/08/09 12:31:16", "commit_message": "Merge remote-tracking branch 'origin' into SPARK-36390", "title": "[SPARK-36390][SQL] Replace SessionState.close with SessionState.detachSession", "body": "### What changes were proposed in this pull request?\r\nSPARK-35286 replace `SessionState.start` with `SessionState.setCurrentSessionState`, but `SessionState.close` will create a `HiveMetaStoreClient` , connect to the Hive Meta Store Server, and then load all functions.\r\n\r\nSPARK-35556 (Remove close HiveClient's SessionState) When the Hive version used is greater than or equal to 2.1, `SessionState.close` is not called and the resource dir of HiveClient is not cleaned up.\r\n\r\n### Why are the changes needed?\r\nClean up the hive resources dir temporary directory.\r\nAvoid wasting resources and accelerate the exit speed.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nadd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}]}{"author": "cxzl25", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}{"author": "cxzl25", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/06/28 04:56:59", "commit_message": "Create hive permanent function with owner name", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}{"author": "cxzl25", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "[SPARK-36550][SQL] Propagation cause when UDF reflection fails", "body": "### What changes were proposed in this pull request?\r\nWhen the exception is InvocationTargetException, get cause and stack trace.\r\n\r\n### Why are the changes needed?\r\nNow when UDF reflection fails, InvocationTargetException is thrown, but it is not a specific exception.\r\n```\r\nError in query: No handler for Hive UDF 'XXX': java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}{"author": "cxzl25", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}{"author": "cxzl25", "commit_date": "2021/06/28 04:56:59", "commit_message": "Create hive permanent function with owner name", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}{"author": "cxzl25", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}{"author": "cxzl25", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}{"author": "cxzl25", "commit_date": "2021/06/28 04:56:59", "commit_message": "Create hive permanent function with owner name", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}{"author": "cxzl25", "commit_date": "2021/06/30 08:33:25", "commit_message": "renameFunction use the original name", "title": "[SPARK-35913][SQL] Create hive permanent function with owner name", "body": "### What changes were proposed in this pull request?\r\nCreate hive permanent function with owner name\r\n\r\n\r\n### Why are the changes needed?\r\nThe hive permanent function created by spark does not have an owner name, while the permanent function created by hive has an owner name\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}{"author": "cxzl25", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}{"author": "cxzl25", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}{"author": "cxzl25", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}{"author": "ekoifman", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins", "body": "### What changes were proposed in this pull request?\r\n\r\nAdd \"num broadcast joins conversions\" and \"num skew join conversions\"\r\nmetrics to AdaptiveSparkPlanExec to report how many joins were changed to BHJ or had skew mitigated due to AQE.\r\n\r\n### Why are the changes needed?\r\n\r\nTo make it easy to get a sense of how much impact AQE had on an a complex query.\r\n\r\nIt's also useful for systems that collect metrics for later analysis of AQE effectiveness in large production deployment.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n2 new SQL metrics will now be emitted\r\n\r\n### How was this patch tested?\r\n\r\nExisting and new Unit tests.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}{"author": "ekoifman", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}{"author": "ekoifman", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}{"author": "ekoifman", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}{"author": "ekoifman", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/09/07 01:48:49", "commit_message": "nit", "title": "[SPARK-36638][SQL] Generalize OptimizeSkewedJoin", "body": "### What changes were proposed in this pull request?\r\nThis PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:\r\n\r\n1, find the _splittable_ shuffle query stages by the semantics of internal nodes;\r\n\r\n2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;\r\n\r\n3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.\r\n\r\n4, attach new specs to shuffle query stages;\r\n\r\n\r\n### Why are the changes needed?\r\nto Generalize OptimizeSkewedJoin \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\ntwo additional configs are added\r\n\r\n\r\n### How was this patch tested?\r\nexisting testsuites, added testsuites, some cases on our productive system\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 0, 9]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization", "body": "### What changes were proposed in this pull request?\r\n1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;\r\n\r\n2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;\r\n\r\n3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.\r\n\r\n\r\n### Why are the changes needed?\r\n1, make it easy to add a new statistics which can be used in AQE rules;\r\n2, showing skew info on sparkUI is usefully;\r\n3, spliting partitions based on joined size can resolve data inflation;\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new features are added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuites\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "zhengruifeng", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "291", "deletions": "136", "changes": "427"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}{"author": "zhengruifeng", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}{"author": "sungpeo", "commit_date": "2021/08/25 07:22:15", "commit_message": "SPARK-36582 Spark HistoryPage show 'NotFound' in not logged multiple attempts\n\nattemptId should be always, so deleted hasMultipleAttempts", "title": "[SPARK-36582][UI] Spark HistoryPage show 'NotFound' in not logged multiple attempts", "body": "### What changes were proposed in this pull request?\r\n\r\n`attemptId` should be always, so deleted `hasMultipleAttempts`\r\n\r\n### Why are the changes needed?\r\n\r\nDescribed in https://issues.apache.org/jira/browse/SPARK-36582\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n#### before change\r\n\r\nWhen all applications in spark history had an attempt only, it doesn't show attemptId\r\n\r\n![Screen Shot 2021-08-26 at 10 47 23 AM](https://user-images.githubusercontent.com/13159599/130886943-36666846-4dca-4687-9e3f-9c6d339207bd.png)\r\n\r\nNow it shows a attemptId column regardless of hasMultipleAttempts.\r\n\r\n![Screen Shot 2021-08-26 at 10 04 43 AM](https://user-images.githubusercontent.com/13159599/130883769-b10916ef-ccaa-4811-8e70-fa27e8a8fceb.png)\r\n\r\n\r\n### How was this patch tested?\r\n\r\nI checked chrome developer tool's console in changed web ui. (History Server's home page)\r\n", "failed_tests": ["org.apache.spark.sql.streaming.FileStreamSinkV2Suite"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage-template.html", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage.js", "additions": "6", "deletions": "17", "changes": "23"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "[SPARK-36604][SQL] timestamp type column stats result consistent with the time zone", "body": "\r\n### What changes were proposed in this pull request?\r\ntimestamp type column stats result should consistent with time zone\r\n\r\n\r\n### Why are the changes needed?\r\nfor now timestamp type column stats result is based on UTC TimeZone\uff0cindependent from the time zone. Thus the stats result may not correct\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd new test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/05/11 08:46:12", "commit_message": "[SPARK-35359][SQL]Insert data with char/varchar datatype will fail when\ndata length exceed length limitation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "[SPARK-36518][Deploy] Spark should support distribute directory to cluster", "body": "\r\n### What changes were proposed in this pull request?\r\nsupport distribute directory to cluster via --files\r\nbefore:\r\n[root@kwephispra41893 spark]# ll /opt/ygh/testdir/\r\ntotal 8\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf\r\n\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)\r\n\r\nafter:\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)\r\n\r\n\r\n### Why are the changes needed?\r\nwhen we submit spark application we can't directly distribute a directory to cluster\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/05/11 08:46:12", "commit_message": "[SPARK-35359][SQL]Insert data with char/varchar datatype will fail when\ndata length exceed length limitation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "fhygh", "commit_date": "2021/05/11 08:46:12", "commit_message": "[SPARK-35359][SQL]Insert data with char/varchar datatype will fail when\ndata length exceed length limitation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}{"author": "Kimahriman", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nI am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:\r\n\r\n```\r\ntransformed = F.regexp_replace(F.lower(F.trim('my_column')))\r\ndf.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))\r\n```\r\nor\r\n```\r\ndf.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))\r\n```\r\n\r\nIn these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.\r\n\r\nIn practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.\r\n\r\nThe only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.\r\n\r\nI also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo increase the performance of conditional expressions.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, just performance improvements.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew and updated UT.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}{"author": "Kimahriman", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}{"author": "Kimahriman", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}{"author": "Kimahriman", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}{"author": "fsamuel-bs", "commit_date": "2021/08/31 17:25:55", "commit_message": "docs", "title": "[SPARK-36627][CORE] Fix java deserialization of proxy classes", "body": "## Upstream SPARK-XXXXX ticket and PR link (if not applicable, explain)\r\nhttps://issues.apache.org/jira/browse/SPARK-36627\r\n\r\n## What changes were proposed in this pull request?\r\nSee issue above for issue description.\r\n\r\n### Why are the changes needed?\r\nSpark deserialization fails with no recourse for the user.\r\n\r\n### Does this PR introduce any user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nUnit tests.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/java/org/apache/spark/serializer/ProxySerializerTest.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/serializer/JavaSerializerSuite.scala", "additions": "24", "deletions": "2", "changes": "26"}, "updated": [0, 0, 0]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/22 14:04:36", "commit_message": "expose localtimestamp in pyspark.sql.functions", "title": "[SPARK-36259] Expose localtimestamp in pyspark.sql.functions", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nExposing localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### Why are the changes needed?\r\nWas previously only available in scala\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nnew localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### How was this patch tested?\r\ntest added inline\r\n", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [2, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "dominikgehl", "commit_date": "2021/07/23 07:38:48", "commit_message": "adding TimestampNTZType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "[WIP][SPARK-36182][SQL] Support TimestampNTZ type in Parquet file source", "body": "This is still WIP. I am deciding the behaviors of the Parquet reader for both schema inference and user-provided schema.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "gengliangwang", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "maropu", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 16:13:58", "commit_message": "fix transparently canonicalized expressions", "title": "[SPARK-36073][SQL] EquivalentExpressions fixes and improvements", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR:\r\n- Fixes the performance issue mentioned in https://github.com/apache/spark/pull/32559/files#r633488455 and partially fixed in https://github.com/apache/spark/pull/33142/files#r660897248 using a new option to remove expressions from equivalence maps.\r\n- Fixes a bug with identifying common expressions in conditional expressions (a side effect of the above new approach). After this PR, `add` will be common subexpression in the following example:\r\n  ```\r\n  val ifExpr1 = If(Literal(true), add, Literal(3))\r\n  val ifExpr3 = If(GreaterThan(add, Literal(4)), Add(ifExpr1, add), Multiply(ifExpr1, add))\r\n  var equivalence = new EquivalentExpressions\r\n  equivalence.addExprTree(ifExpr3)\r\n  ```\r\n- Fixes a bug of transparently canonicalized expressions (like `PromotePrecision`) are considered common subexpressions.\r\n  After this PR, `transparent` will not be common subexpression in the following example:\r\n  ```\r\n  val add = Add(Literal(1), Literal(2))\r\n  val transparent = PromotePrecision(add)\r\n  var equivalence = new EquivalentExpressions\r\n  equivalence.addExprTree(transparent)\r\n  ```\r\n\r\n### Why are the changes needed?\r\nBugfix + performance improvement.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nExisting + new UTs.\r\n\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "peter-toth", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}{"author": "peter-toth", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "112", "deletions": "552", "changes": "664"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "30", "deletions": "140", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}{"author": "cutiechi", "commit_date": "2021/07/28 02:26:55", "commit_message": "Fix executor pod create or replace NPE", "title": "[SPARK-36039][K8S] Fix executor pod hadoop conf mount", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nFix executor pod hadoop conf mount.\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nArg --conf spark.kubernetes.hadoop.configMapName for executor pod not working.\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nUT.\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/HadoopConfExecutorFeatureStep.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/HadoopConfExecutorFeatureStepSuite.scala", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 0, 0]}]}{"author": "kyoty", "commit_date": "2021/07/13 17:44:07", "commit_message": "Bump addressable from 2.7.0 to 2.8.0 in /docs\n\nBumps [addressable](https://github.com/sporkmonger/addressable) from 2.7.0 to 2.8.0.\n- [Release notes](https://github.com/sporkmonger/addressable/releases)\n- [Changelog](https://github.com/sporkmonger/addressable/blob/main/CHANGELOG.md)\n- [Commits](https://github.com/sporkmonger/addressable/compare/addressable-2.7.0...addressable-2.8.0)\n\n---\nupdated-dependencies:\n- dependency-name: addressable\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/Gemfile.lock", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "kyoty", "commit_date": "2021/07/13 17:44:07", "commit_message": "Bump addressable from 2.7.0 to 2.8.0 in /docs\n\nBumps [addressable](https://github.com/sporkmonger/addressable) from 2.7.0 to 2.8.0.\n- [Release notes](https://github.com/sporkmonger/addressable/releases)\n- [Changelog](https://github.com/sporkmonger/addressable/blob/main/CHANGELOG.md)\n- [Commits](https://github.com/sporkmonger/addressable/compare/addressable-2.7.0...addressable-2.8.0)\n\n---\nupdated-dependencies:\n- dependency-name: addressable\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/Gemfile.lock", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.\r\n\r\n**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThese changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, this PR introduces a set of new APIs for Data Source V2.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nThis PR comes with a trivial test. More tests to come.\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "aokolnychyi", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.ReplaceNullWithFalseInPredicateSuite", "org.apache.spark.sql.catalyst.optimizer.PullupCorrelatedPredicatesSuite", "org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalsInPredicateSuite", "org.apache.spark.sql.execution.command.PlanResolutionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}{"author": "dnskr", "commit_date": "2021/08/14 10:33:16", "commit_message": "Merge branch 'apache:master' into docs/SPARK-36510", "title": "[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs", "body": "### What changes were proposed in this pull request?\r\nThe PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs\r\n\r\n### Why are the changes needed?\r\nThe property referred by `spark.sql.redaction.string.regex` description as its default value\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot needed for docs\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/configuration.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}{"author": "wankunde", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "wankunde", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "wankunde", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "[SPARK-35713]Bug fix for thread leak in JobCancellationSuite", "body": "### What changes were proposed in this pull request?\r\n\r\nBug fix for thread leak in JobCancellationSuite UT\r\n\r\n### Why are the changes needed?\r\n\r\nWhen we call Thread.interrupt() method, that thread's interrupt status will be set but it may not really interrupt.\r\nSo when spark task runs in an infinite loop, spark context may fail to interrupt the task thread, and resulting in thread leak. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nTest case \"task reaper kills JVM if killed tasks keep running for too long\" in JobCancellationSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "wankunde", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}{"author": "brandondahler", "commit_date": "2021/06/14 15:09:40", "commit_message": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "title": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "body": "### What changes were proposed in this pull request?\r\nAdds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).\r\n\r\n### Why are the changes needed?\r\nImproved development experience for developers using Spark SQL, specifically when coding in Java.  \r\n\r\nPrior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.\r\n\r\n### How was this patch tested?\r\nTest cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.\r\n\r\n* Changes were tested in Scala via spark-shell.\r\n* Changes were tested in Java by modifying an example:\r\n  ```\r\n  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  index 86a9045d8a..342810c1e6 100644\r\n  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {\r\n       // |-- age: long (nullable = true)\r\n       // |-- name: string (nullable = true)\r\n\r\n  +    df.join(df, new String[] {\"age\"}).show();\r\n  +    df.join(df, \"age\", \"left\").show();\r\n  +    df.join(df, new String[] {\"age\"}, \"left\").show();\r\n  +\r\n       // Select only the \"name\" column\r\n       df.select(\"name\").show();\r\n       // +-------+\r\n  ```", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "79", "deletions": "2", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}{"author": "sigmod", "commit_date": "2021/06/17 02:29:39", "commit_message": "fix ResolveWriteToStream", "title": "[WIP][SPARK-35724][SQL] Support traversal pruning in extendedResolutionRules and postHocResolutionRules", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\n\r\nExisting tests.\r\nPerformance diff (org.apache.spark.sql.TPCDSQuerySuite):\r\n<google-sheets-html-origin><style type=\"text/css\"><!--td {border: 1px solid #ccc;}br {mso-data-placement:same-cell;}--></style>\r\n&nbsp; | Baseline | Experiment | Experiment/Baseline\r\n-- | -- | -- | --\r\norg.apache.spark.sql.execution.aggregate.ResolveEncodersInScalaAgg | 61864479 | 2072405 | 0.03\r\norg.apache.spark.sql.execution.datasources.DataSourceAnalysis | 4186305 | 3876249 | 0.93\r\norg.apache.spark.sql.execution.datasources.FallBackFileSourceV2 | 7665256 | 3190492 | 0.42\r\norg.apache.spark.sql.execution.datasources.FindDataSourceTable | 145262307 | 139461170 | 0.96\r\norg.apache.spark.sql.execution.datasources.PreprocessTableInsertion | 2248277 | 1424614 | 0.63\r\norg.apache.spark.sql.execution.datasources.ResolveSQLOnFile | 8633729 | 8849599 | 1.03\r\norg.apache.spark.sql.execution.datasources.SchemaPruning | 200094008 | 5550433 | 0.03\r\norg.apache.spark.sql.execution.dynamicpruning.PartitionPruning | 70860220 | 66498442 | 0.94\r\norg.apache.spark.sql.execution.OptimizeMetadataOnlyQuery | 1489644 | 1632468 | 1.10\r\norg.apache.spark.sql.execution.streaming.ResolveWriteToStream | 9494697 | 3030265 | 0.32\r\n\r\n</google-sheets-html-origin>\r\n", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite", "org.apache.spark.sql.hive.thriftserver.CliSuite", "org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.ml.feature.ElementwiseProductSuite", "org.apache.spark.ml.classification.DecisionTreeClassifierSuite", "org.apache.spark.ml.feature.PCASuite", "org.apache.spark.ml.classification.LinearSVCSuite", "org.apache.spark.ml.feature.StandardScalerSuite", "org.apache.spark.ml.feature.MaxAbsScalerSuite", "org.apache.spark.ml.clustering.LDASuite", "org.apache.spark.ml.classification.LogisticRegressionSuite", "org.apache.spark.ml.feature.DCTSuite", "org.apache.spark.ml.regression.IsotonicRegressionSuite", "org.apache.spark.ml.feature.BucketizerSuite", "org.apache.spark.ml.clustering.BisectingKMeansSuite", "org.apache.spark.ml.util.MLTestSuite", "org.apache.spark.ml.regression.AFTSurvivalRegressionSuite", "org.apache.spark.ml.tuning.CrossValidatorSuite", "org.apache.spark.ml.feature.OneHotEncoderSuite", "org.apache.spark.ml.feature.RobustScalerSuite", "org.apache.spark.ml.feature.VectorIndexerSuite", "org.apache.spark.ml.feature.BucketedRandomProjectionLSHSuite", "org.apache.spark.ml.classification.GBTClassifierSuite", "org.apache.spark.ml.tuning.TrainValidationSplitSuite", "org.apache.spark.ml.recommendation.ALSSuite", "org.apache.spark.ml.clustering.GaussianMixtureSuite", "org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite", "org.apache.spark.ml.feature.VarianceThresholdSelectorSuite", "org.apache.spark.ml.regression.LinearRegressionSuite", "org.apache.spark.ml.feature.VectorSizeHintSuite", "org.apache.spark.ml.feature.HashingTFSuite", "org.apache.spark.ml.classification.NaiveBayesSuite", "org.apache.spark.ml.feature.Word2VecSuite", "org.apache.spark.ml.feature.MinHashLSHSuite", "org.apache.spark.ml.feature.ImputerSuite", "org.apache.spark.ml.feature.SQLTransformerSuite", "org.apache.spark.ml.feature.MinMaxScalerSuite", "org.apache.spark.ml.regression.DecisionTreeRegressorSuite", "org.apache.spark.ml.feature.VectorSizeHintStreamingSuite", "org.apache.spark.ml.feature.NGramSuite", "org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite", "org.apache.spark.ml.feature.UnivariateFeatureSelectorSuite", "org.apache.spark.ml.feature.CountVectorizerSuite", "org.apache.spark.ml.feature.RFormulaSuite", "org.apache.spark.ml.feature.QuantileDiscretizerSuite", "org.apache.spark.ml.feature.NormalizerSuite", "org.apache.spark.ml.regression.GBTRegressorSuite", "org.apache.spark.ml.feature.InteractionSuite", "org.apache.spark.ml.feature.RegexTokenizerSuite", "org.apache.spark.ml.feature.ChiSqSelectorSuite", "org.apache.spark.ml.clustering.KMeansSuite", "org.apache.spark.ml.feature.PolynomialExpansionSuite", "org.apache.spark.ml.feature.StopWordsRemoverSuite", "org.apache.spark.ml.classification.OneVsRestSuite", "org.apache.spark.ml.feature.VectorSlicerSuite", "org.apache.spark.ml.classification.RandomForestClassifierSuite", "org.apache.spark.ml.feature.StringIndexerSuite", "org.apache.spark.ml.feature.BinarizerSuite", "org.apache.spark.ml.feature.IDFSuite", "org.apache.spark.ml.feature.FeatureHasherSuite", "org.apache.spark.ml.classification.FMClassifierSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTrackerMetricSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/streaming/WriteToStreamStatement.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/analysis/DetectAmbiguousSelfJoin.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FallBackFileSourceV2.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ResolveWriteToStream.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}]}{"author": "sigmod", "commit_date": "2021/06/11 08:46:43", "commit_message": "update", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite", "org.apache.spark.sql.hive.thriftserver.CliSuite", "org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.ml.feature.ElementwiseProductSuite", "org.apache.spark.ml.classification.DecisionTreeClassifierSuite", "org.apache.spark.ml.feature.PCASuite", "org.apache.spark.ml.classification.LinearSVCSuite", "org.apache.spark.ml.feature.StandardScalerSuite", "org.apache.spark.ml.feature.MaxAbsScalerSuite", "org.apache.spark.ml.clustering.LDASuite", "org.apache.spark.ml.classification.LogisticRegressionSuite", "org.apache.spark.ml.feature.DCTSuite", "org.apache.spark.ml.regression.IsotonicRegressionSuite", "org.apache.spark.ml.feature.BucketizerSuite", "org.apache.spark.ml.clustering.BisectingKMeansSuite", "org.apache.spark.ml.util.MLTestSuite", "org.apache.spark.ml.regression.AFTSurvivalRegressionSuite", "org.apache.spark.ml.tuning.CrossValidatorSuite", "org.apache.spark.ml.feature.OneHotEncoderSuite", "org.apache.spark.ml.feature.RobustScalerSuite", "org.apache.spark.ml.feature.VectorIndexerSuite", "org.apache.spark.ml.feature.BucketedRandomProjectionLSHSuite", "org.apache.spark.ml.classification.GBTClassifierSuite", "org.apache.spark.ml.tuning.TrainValidationSplitSuite", "org.apache.spark.ml.recommendation.ALSSuite", "org.apache.spark.ml.clustering.GaussianMixtureSuite", "org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite", "org.apache.spark.ml.feature.VarianceThresholdSelectorSuite", "org.apache.spark.ml.regression.LinearRegressionSuite", "org.apache.spark.ml.feature.VectorSizeHintSuite", "org.apache.spark.ml.feature.HashingTFSuite", "org.apache.spark.ml.classification.NaiveBayesSuite", "org.apache.spark.ml.feature.Word2VecSuite", "org.apache.spark.ml.feature.MinHashLSHSuite", "org.apache.spark.ml.feature.ImputerSuite", "org.apache.spark.ml.feature.SQLTransformerSuite", "org.apache.spark.ml.feature.MinMaxScalerSuite", "org.apache.spark.ml.regression.DecisionTreeRegressorSuite", "org.apache.spark.ml.feature.VectorSizeHintStreamingSuite", "org.apache.spark.ml.feature.NGramSuite", "org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite", "org.apache.spark.ml.feature.UnivariateFeatureSelectorSuite", "org.apache.spark.ml.feature.CountVectorizerSuite", "org.apache.spark.ml.feature.RFormulaSuite", "org.apache.spark.ml.feature.QuantileDiscretizerSuite", "org.apache.spark.ml.feature.NormalizerSuite", "org.apache.spark.ml.regression.GBTRegressorSuite", "org.apache.spark.ml.feature.InteractionSuite", "org.apache.spark.ml.feature.RegexTokenizerSuite", "org.apache.spark.ml.feature.ChiSqSelectorSuite", "org.apache.spark.ml.clustering.KMeansSuite", "org.apache.spark.ml.feature.PolynomialExpansionSuite", "org.apache.spark.ml.feature.StopWordsRemoverSuite", "org.apache.spark.ml.classification.OneVsRestSuite", "org.apache.spark.ml.feature.VectorSlicerSuite", "org.apache.spark.ml.classification.RandomForestClassifierSuite", "org.apache.spark.ml.feature.StringIndexerSuite", "org.apache.spark.ml.feature.BinarizerSuite", "org.apache.spark.ml.feature.IDFSuite", "org.apache.spark.ml.feature.FeatureHasherSuite", "org.apache.spark.ml.classification.FMClassifierSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTrackerMetricSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 4, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 3, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/streaming/WriteToStreamStatement.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [1, 3, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/analysis/DetectAmbiguousSelfJoin.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FallBackFileSourceV2.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ResolveWriteToStream.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 2]}]}{"author": "robert3005", "commit_date": "2017/06/22 15:53:30", "commit_message": "Merge existing registry with default one or configure default metric registry", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "66", "deletions": "19", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "30", "deletions": "9", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/StreamingContextSuite.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}]}{"author": "q2w", "commit_date": "2021/06/14 10:15:32", "commit_message": "add config to put migrating blocks on disk only", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionUnitSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 2, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerDecommissioner.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionUnitSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 4, 5]}]}{"author": "q2w", "commit_date": "2021/06/28 03:45:20", "commit_message": "fix UT faling in BlockManagerDecommissionUnitSuite because of change in number of arguments in replicateBlocks method in BlockManager", "title": "[SPARK-35754][CORE] Add config to put migrating blocks on disk only", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR adds a config which makes block manager decommissioner to migrate block data on disk only. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nWhile migrating block data, if enough memory is not available on peer block managers existing blocks are dropped. After this PR migrating blocks won't drop any existing blocks. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUT in BlockManagerSuite", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionUnitSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerDecommissioner.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionUnitSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 1, 5]}]}{"author": "q2w", "commit_date": "2021/06/14 10:15:32", "commit_message": "add config to put migrating blocks on disk only", "title": "", "body": "", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionUnitSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 2, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerDecommissioner.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionUnitSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 4, 5]}]}{"author": "zheniantoushipashi", "commit_date": "2021/07/03 07:23:45", "commit_message": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "title": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\n\r\nThe canCast method of type of char/varchar is modified to be consistent with StringType\r\n\r\n\r\nthe method cast will change the type char/varchar to StringType \r\n\r\n  def cast(to: DataType): Column = withExpr {\r\n    val cast = Cast(expr, CharVarcharUtils.replaceCharVarcharWithStringForCast(to))\r\n    cast.setTagValue(Cast.USER_SPECIFIED_CAST, true)\r\n    cast\r\n  }\r\n\r\n\r\nThe canCast method of type of char/varchar  must   be consistent with StringType\r\n\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nBefore I used stringType instead of char/varchar, my application code has the logic to judge using canCast. There was no problem before, but now it\u2019s changed to char/varchar, and the judgment of canCast fails. If it doesn\u2019t pass, I Need to change a lot of application code\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nno\r\n\r\n\r\n### How was this patch tested?\r\n\r\ni add UT\u3002\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 5, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 5, 16]}]}{"author": "zhongyu09", "commit_date": "2021/05/16 15:10:12", "commit_message": "SPARK-35414: Submit broadcast job first to avoid broadcast timeout in AQE", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.DynamicPartitionPruningSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "34", "deletions": "1", "changes": "35"}, "updated": [0, 0, 4]}]}{"author": "grarkydev", "commit_date": "2021/04/06 11:44:03", "commit_message": "Support spark application managing with spark app handle on kubernetes\n\nCo-authored-by: hongdd <hongdongdong@cmss.chinamobile.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "40", "deletions": "3", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcher.scala", "additions": "34", "deletions": "5", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "21", "deletions": "8", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcherSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}{"author": "shardulm94", "commit_date": "2021/07/22 21:07:10", "commit_message": "Fix compile error after revert", "title": "[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd logging to `ShuffleBlockFetcherIterator` to log \"slow\" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is \"slow\".\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nAdds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}{"author": "shardulm94", "commit_date": "2021/07/20 01:44:48", "commit_message": "SPARK-36215: Add logging for slow fetches to diagnose external shuffle service issues\n\nCo-authored-by: Shardul Mahadik <smahadik@linkedin.com>", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}{"author": "Swinky", "commit_date": "2021/07/12 04:39:27", "commit_message": "Merge branch 'swmann/dpp-subquery-exprid' of https://github.com/Swinky/spark into swmann/dpp-subquery-exprid", "title": "[SPARK-35911][SQL] Update exprId for IN subquery in DPP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nUpdate exprId for IN subquery for DPP in executed plan; to have same expr Id as DynamicPruning filter in optimized plan.\r\n\r\n\r\n### Why are the changes needed?\r\nThis minor change shall make debugging easier in complex queries.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, just exprId changes in the optimized and executed plans. Now both shall have same exprId for an DPP expression/subquery.\r\n\r\n\r\n### How was this patch tested?\r\nAdded a check in existing UTs.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 1, 4]}]}{"author": "shipra-a", "commit_date": "2021/06/09 01:35:41", "commit_message": "https://github.com/apache/spark/pull/28804/commits\n\nCo-authored-by: Karuppayya Rajendran <karuppayya1990@gmail.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "206", "deletions": "49", "changes": "255"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "45", "deletions": "41", "changes": "86"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala", "additions": "34", "deletions": "27", "changes": "61"}, "updated": [0, 0, 0]}]}{"author": "kbendick", "commit_date": "2021/06/23 04:39:14", "commit_message": "Increase stack size in build with maven", "title": "", "body": "", "failed_tests": ["pyspark.streaming.tests.test_context"], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 26]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 8, 32]}]}