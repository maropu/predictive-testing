name: Build Spark class dependency graphs

on:
  workflow_dispatch:
  schedule:
   - cron: '0 0 * * 0'
jobs:
  build-spark-dep-graphs:
    runs-on: ubuntu-latest
    env:
      python: 3.6
      java: 1.8
    strategy:
      fail-fast: false
    steps:
      - name: Checkout predictive-testing repository
        uses: actions/checkout@v2
        # In order to fetch changed files
        with:
          fetch-depth: 0
      - name: Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: master
          path: spark-master
      - name: Generate output name by using Spark HEAD commit sha
        run: |
          OUTPUT_NAME=spark-dep-graphs-`date '+%Y%m%d%H%M'`-`git rev-parse --short HEAD`
          echo "OUTPUT_NAME=${OUTPUT_NAME}" >> $GITHUB_ENV
      - name: Install Python ${{ env.python }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ env.python }}
          architecture: x64
      - name: Install Python packages (Python ${{ env.python }})
        run: python -m pip install -r ./bin/requirements.txt
      - name: Install JDK ${{ env.java }}
        uses: actions/setup-java@v1
        with:
          java-version: ${{ env.java }}
      - name: Build with Maven
        run: |
          export MAVEN_OPTS="-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g -Dorg.slf4j.simpleLogger.defaultLogLevel=WARN"
          export MAVEN_CLI_OPTS="--no-transfer-progress"
          export JAVA_VERSION=${{ env.java }}
          cd spark-master && ./build/mvn $MAVEN_CLI_OPTS -DskipTests -Pyarn -Pmesos -Pkubernetes -Phive -Phive-thriftserver -Phadoop-cloud -Djava.version=$JAVA_VERSION test-compile
      - name: Analyze Spark class dependencies
        run: |
          ./bin/analyze-spark-deps.sh ./spark-master "${{ env.OUTPUT_NAME }}"

      # TODO: Needs to handle Python files in `./bin/analyze-spark-deps.sh`, too
      - name: List up Python test files
        run: |
          OUTPUT_PATH=`pwd`/${{ env.OUTPUT_NAME }}
          cd ./spark-master/python && find . -type f | grep -E "test_[a-zA-Z0-9_-]*\.py$" | sed -r 's/.*(pyspark\/.*\/test_.*)\.py$/\1/' > $OUTPUT_NAME/test-python-files.lst
      - name: Upload output as artifact
        if: success()
        uses: actions/upload-artifact@v2
        with:
          name: spark-dep-graphs
          path: ${{ env.OUTPUT_NAME }}
      - name: Locate output in models/spark/indexes/
        run: |
          mv "${{ env.OUTPUT_NAME }}" models/spark/indexes/
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: Adds `models/spark/indexes/${{ env.OUTPUT_NAME }}`
          committer: GitHub <noreply@github.com>
          author: ${{ github.actor }} <${{ github.actor }}@users.noreply.github.com>
          signoff: false
          branch: ${{ github.job }}-${{ env.OUTPUT_NAME }}
          delete-branch: true
          title: 'Adds `models/spark/indexes/${{ env.OUTPUT_NAME }}`'
          body: |
            Automated changes by the `${{ github.job }}` workflow (run_id=${{ github.run_id }}).
