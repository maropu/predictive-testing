[["34528", "2021-11-09T05:17:59Z", "2021-11-10T08:23:46Z", "[SPARK-37202][SQL] Treat `injectedFunctions` as custom built-in functions", "### What changes were proposed in this pull request?\r\nThis PR fixes an issue that the `injectedFunctions` in `SparkSessionExtensions` can't be\r\nresolved correctly if it's referred by a temporary view. This is because the injected functions\r\nare not `UserDefinedExpression`. So they will be treated as temporary functions but couldn't\r\nbe captured as temporary functions during view creation. As a result, the function resolution\r\nwill fail if it's reffered by a temp view.\r\n\r\nThis PR adds a new concept `customBuiltinFunction`, so that the injected functions will be\r\ntreated as builtin functions intead of tempoary ones. With with way, it's not needed to be\r\ncaptured by temp view anymore.\r\n\r\n### Why are the changes needed?\r\nbug fix\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nAfter this PR, the `SparkSessionExtensions.injectedFunctions` are not temporary functions anymore.\r\n```scala\r\nval extensions = create { extensions =>\r\n  extensions.injectFunction(MyExtensions.myFunction)\r\n}\r\nwithSession(extensions) { session =>\r\n  // return `false` after this PR, and `true` before this PR\r\n  session.sessionState.catalog.isTemporaryFunction(MyExtensions.myFunction._1)\r\n}\r\n```\r\n\r\n\r\n### How was this patch tested?\r\nnewly added test", "linhongliu-db", "spark", "SPARK-37202_1"], ["34541", "2021-11-10T05:06:03Z", "2021-11-10T08:21:59Z", "[SPARK-37264][BUILD] Exclude `hadoop-client-api` transitive dependency from `orc-core`", "### What changes were proposed in this pull request?\r\n\r\nLike `hadoop-common` and `hadoop-hdfs`, this PR proposes to exclude `hadoop-client-api` transitive dependency from `orc-core`.\r\n\r\n### Why are the changes needed?\r\n\r\nSince Apache Hadoop 2.7 doesn't work on Java 17, Apache ORC has a dependency on Hadoop 3.3.1.\r\nThis causes `test-dependencies.sh` failure on Java 17 on `hadoop-2.7` profile.  As a result, `run-tests.py` also fails.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nConfirmed that `test-dependencyies.sh` works on Java 17 correctly because the transitive dependency is cut.\r\n```\r\nJAVA_HOME=/path/to/java17/ build/mvn -Phadoop-2.7 dependency:tree\r\n...\r\n[INFO] +- org.apache.orc:orc-core:jar:1.7.1:compile\r\n[INFO] |  +- org.apache.orc:orc-shims:jar:1.7.1:compile\r\n[INFO] |  +- com.google.protobuf:protobuf-java:jar:2.5.0:compile\r\n[INFO] |  +- io.airlift:aircompressor:jar:0.21:compile\r\n[INFO] |  +- org.jetbrains:annotations:jar:17.0.0:compile\r\n[INFO] |  \\- org.threeten:threeten-extra:jar:1.5.0:compile\r\n\r\n...\r\n```", "sarutak", "spark", "disable-test-dependencies-for-java17"], ["34495", "2021-11-05T13:29:50Z", "2021-11-10T08:06:07Z", "[SPARK-36182][SQL] Support TimestampNTZ type in Parquet data source ", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSupport TimestampNTZ type in Parquet data source. In this PR, the [timestamp types](https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp) of Parquet are mapped to the two timestamp types in Spark:\r\n<table>\r\n    <tr colspan=\"3\">\r\n        <th colspan=\"3\">Parquet type</th>\r\n        <th>Spark Catalyst Type</th>\r\n    </tr>\r\n    <tr>\r\n        <td rowspan=\"4\">INT64 Timestamp</td>\r\n        <td rowspan=\"2\">isAdjustedToUTC = false</td>\r\n        <td>unit = MILLIS</td>\r\n        <td>TimestampNTZType</td>\r\n    </tr>\r\n    <tr>\r\n        <td>unit = MICROS</td>\r\n        <td>TimestampNTZType</td>\r\n    </tr>\r\n    <tr>\r\n        <td rowspan=\"2\">isAdjustedToUTC = true</td>\r\n        <td>unit = MILLIS</td>\r\n        <td>TimestampType</td>\r\n    </tr>\r\n    <tr>\r\n        <td>unit = MICROS</td>\r\n        <td>TimestampType</td>\r\n    </tr>\r\n    <tr>\r\n        <td>INT96 Timestamp</td>\r\n        <td> - </td>\r\n        <td> - </td>\r\n        <td> TimestampType </td>\r\n</table>\r\n\r\n#### Parquet writer\r\nFor TIMESTAMP_NTZ columns, it follows the Parquet Logical Type Definitions and sets the field `isAdjustedToUTC` as `false` on writing TIMESTAMP_NTZ columns. The output type is always INT64 in the MICROS time unit. Parquet's timestamp logical annotation can only be used for INT64 so that we won't support writing TIMESTAMP_NTZ as INT96. Otherwise, it is hard to decide the timestamp type on the reader side.\r\n\r\n#### Parquet reader\r\n\r\n- INT96 columns: The reader behavior is the same with Spark 3.2 or prior.\r\n- Schema inference for INT64 Timestamp columns: for schema inference of one file, Spark infers TIMESTAMP_NTZ or TIMESTAMP_LTZ type according to the annotation flag `isAdjustedToUTC`.\r\n- Row converter for INT64 Timestamp columns during reading\r\n    - Given a TIMESTAMP_NTZ Parquet column and a catalyst schema of TIMESTAMP_LTZ type, Spark **allows** the read \r\n       operation since TIMESTAMP_LTZ is the \"wider\" type.\r\n    - Given a TIMESTAMP_LTZ Parquet column and a catalyst schema of TIMESTAMP_NTZ type, Spark **disallows** the read operation since TIMESTAMP_NTZ is the \"narrower\" type.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n Support TimestampNTZ type in Parquet data source \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, support TimestampNTZ type in Parquet data source \r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nNew UTs", "gengliangwang", "spark", "parquetTSNTZ"], ["34535", "2021-11-09T14:34:53Z", "2021-11-10T07:33:13Z", "[SPARK-37201][SQL] GeneratorNestedColumnAliasing support Generate with Filter", "### What changes were proposed in this pull request?\r\nIn current ` GeneratorNestedColumnAliasing`, spark only support  push down case with Project as below\r\n```\r\nProject [v1#225, el#226]\r\n   +- Project [struct#220.v1 AS v1#225, el#226, struct#220]\r\n      +- Generate explode(array#221), false, [el#226]\r\n         +- SubqueryAlias spark_catalog.default.table\r\n            +- Relation default.table[struct#220,array#221] parquet\r\n```\r\n\r\nIn this pr we support push dow with Project and Filter as below\r\n```\r\nProject [v1#225, el#226]\r\n +- Project [struct#220.v1 AS v1#225, el#226, struct#220]\r\n    +- Filter ((el#226 = cx1) AND (struct#220.v2 = v3))\r\n      +- Generate explode(array#221), false, [el#226]\r\n         +- SubqueryAlias spark_catalog.default.table\r\n            +- Relation default.table[struct#220,array#221] parquet\r\n```\r\n\r\n### Why are the changes needed?\r\nImprove GeneratorNestedColumnAliasing to support more case\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd UT\r\n", "AngersZhuuuu", "spark", "SPARK-37201"], ["34536", "2021-11-09T15:13:49Z", "2021-11-10T07:24:55Z", "[SPARK-35011][CORE] Fix false active executor in UI that caused by BlockManager reregistration  ", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nAlso post the event `SparkListenerExecutorRemoved` when removing an executor, which is known by `BlockManagerMaster` but unknown to `SchedulerBackend`.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nIn https://github.com/apache/spark/pull/32114, it reports an issue that `BlockManagerMaster` could register a `BlockManager` from a dead executor due to reregistration mechanism. The side effect is, the executor will be shown on the UI as an active one, though it's already dead indeed.\r\n\r\nIn https://github.com/apache/spark/pull/32114, we tried to avoid such reregistration for a to-be-dead executor. However, I just realized that we can actually leave such reregistration alone since `HeartbeatReceiver.expireDeadHosts` should clean up those `BlockManager`s in the end. The problem is, the corresponding executors in UI can't be cleaned along with the `BlockManager`s cleaning. Because executors in UI can only be cleaned by `SparkListenerExecutorRemoved`, \r\n while `BlockManager`s  cleaning only post `SparkListenerBlockManagerRemoved` (which is ignored by `AppStatusListener`). \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, users would see the false active executor be removed in the end.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nPass existing tests.", "Ngone51", "spark", "SPARK-35011"], ["33872", "2021-08-31T08:15:36Z", "2021-11-10T07:24:07Z", "[SPARK-36575][CORE] Should ignore task finished event if its task set is gone in TaskSchedulerImpl.handleSuccessfulTask", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWhen a executor finished a task of some stage, the driver will receive a `StatusUpdate` event to handle it. At the same time the driver found the executor heartbeat timed out, so the dirver also need handle ExecutorLost event simultaneously. There was a race condition issues here, which will make `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result.\r\n\r\nThe problem is that `TaskResultGetter.enqueueSuccessfulTask` use asynchronous thread to handle successful task, that mean the synchronized lock of `TaskSchedulerImpl` was released prematurely during midway https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L61. So `TaskSchedulerImpl` may handle executorLost first, then the asynchronous thread will go on to handle successful task. It cause `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n It will cause `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd a new test.", "sleep1661", "spark", "SPARK-36575"], ["34537", "2021-11-09T18:19:32Z", "2021-11-10T07:03:47Z", "[SPARK-37261][SQL] Allow adding partitions with ANSI intervals in DSv2", "### What changes were proposed in this pull request?\r\nIn the PR, I propose to skip checking of ANSI interval types while creating or writing to a table using V2 catalogs. As the consequence of that, users can creating tables in V2 catalogs partitioned by ANSI interval columns (the legacy intervals of `CalendarIntervalType` are still prohibited). Also this PR adds new test which checks:\r\n1. Adding new partition with ANSI intervals via `ALTER TABLE .. ADD PARTITION`\r\n2. INSERT INTO a table partitioned by ANSI intervals \r\n\r\nfor V1/V2 In-Memory catalogs (skips V1 Hive external catalog).\r\n\r\n### Why are the changes needed?\r\nTo allow users saving of ANSI intervals as partition values using DSv2.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nBy running new test for V1/V2 In-Memory and V1 Hive external catalogs:\r\n```\r\n$ build/sbt \"test:testOnly org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite\"\r\n$ build/sbt \"test:testOnly org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite\"\r\n$ build/sbt -Phive-2.3 -Phive-thriftserver \"test:testOnly org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite\"\r\n$ build/sbt -Phive-2.3 -Phive-thriftserver \"test:testOnly *DataSourceV2SQLSuite\"\r\n```", "MaxGekk", "spark", "alter-table-ansi-interval"], ["34213", "2021-10-07T11:50:08Z", "2021-11-10T07:03:09Z", "[SPARK-36396][PYTHON] Implement DataFrame.cov", "### What changes were proposed in this pull request?\r\n\r\nImplement DataFrame.cov\r\n\r\n### Why are the changes needed?\r\n\r\nIncrease pandas API coverage in PySpark\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can use\r\n\r\n``` python\r\n>>> psdf = ps.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\r\n...                   columns=['dogs', 'cats'])\r\n>>> psdf.cov()\r\n       dogs      cats\r\ndogs  0.666667 -1.000000\r\ncats -1.000000  1.666667\r\n\r\n>>> pdf = pd.DataFrame(\r\n...     {\r\n...         \"a\": [1, np.nan, 3, 4],\r\n...         \"b\": [True, False, False, True],\r\n...         \"c\": [True, True, False, True],\r\n...     }\r\n... )\r\n>>> psdf = ps.from_pandas(pdf)\r\n>>> psdf.cov()\r\n          a         b         c\r\na  2.333333 -0.166667 -0.166667\r\nb -0.166667  0.333333  0.166667\r\nc -0.166667  0.166667  0.250000\r\n```\r\n\r\n### How was this patch tested?\r\n\r\nunit tests", "dchvn", "spark", "SPARK-36396"], ["34453", "2021-11-01T03:29:59Z", "2021-11-10T06:17:02Z", "[WIP][SPARK-37173][SQL] SparkGetFunctionOperation return builtin function only once", "### What changes were proposed in this pull request?\r\nAccording to https://github.com/apache/spark/pull/25252/files#r738489764, if we use wild pattern, it will return too much rows.\r\n\r\nIn this pr we return common builtin functions only once\r\n\r\n### Why are the changes needed?\r\nImprove performance\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nWIP\r\n", "AngersZhuuuu", "spark", "SPARK-37173"], ["34473", "2021-11-03T04:56:48Z", "2021-11-10T05:33:49Z", "[SPARK-37202][SQL] Temp view should collect temp functions registered with catalog API", "### What changes were proposed in this pull request?\r\nAdd a new function wrapper called `RegisteredSimpleFunction`, so that we can identify\r\nthe functions among all the expressions. Because of this, when creating temp views,\r\nwe can collect all temporary functions created both by SQL or catalog API.\r\n\r\n### Why are the changes needed?\r\nfix regression. After we choose to store the sql text instead of a logical plan for a view, we couldn't\r\ncollect the temporary functions registered by catalog API. For example:\r\n```scala\r\nval func = CatalogFunction(FunctionIdentifier(\"tempFunc\", None), ...)\r\nval builder = (e: Seq[Expression]) => e.head\r\nspark.sessionState.catalog.registerFunction(func, Some(builder))\r\nsql(\"create temp view tv as select temp1(a, b) from values (1, 2) t(a, b)\")\r\nsql(\"select * from tv\").collect()\r\n```\r\nWe will get an exception for the above code:\r\n```\r\nUndefined function: 'tempFunc'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.;\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, it's a bug-fix.\r\n\r\n\r\n### How was this patch tested?\r\nnewly added UT\r\n", "linhongliu-db", "spark", "SPARK-37202"], ["34538", "2021-11-09T19:32:16Z", "2021-11-10T05:19:36Z", "[SPARK-37221][SQL][FOLLOWUP] Add toRowBased to SparkPlan", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis is a follow up of #34499. Instead of adding `ColumnarToRowExec` in `getByteArrayRdd`, this patch adds `toRowBased` API to explicitly ask for columnar-to-row-based conversion. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nTo make the conversion selectable.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExisting tests.", "viirya", "spark-1", "columnar-followup"], ["34540", "2021-11-09T23:32:59Z", "2021-11-10T05:03:09Z", "[SPARK-37262][SQL] Not log empty aggregate and group by in JDBCScan", "\r\n\r\n### What changes were proposed in this pull request?\r\nCurrently, the empty pushed aggregate and pushed group by are logged in Explain for JDBCScan\r\n```\r\nScan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@72e75786 [NAME#1,SALARY#2] PushedAggregates: [], PushedFilters: [IsNotNull(SALARY), GreaterThan(SALARY,100.00)], PushedGroupby: [], ReadSchema: struct<NAME:string,SALARY:decimal(20,2)>\r\n```\r\n\r\nAfter the fix, the JDBCSScan will be\r\n```\r\nScan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@72e75786 [NAME#1,SALARY#2] PushedFilters: [IsNotNull(SALARY), GreaterThan(SALARY,100.00)], ReadSchema: struct<NAME:string,SALARY:decimal(20,2)>\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\naddress this comment https://github.com/apache/spark/pull/34451#discussion_r740220800\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno\r\n\r\n\r\n### How was this patch tested?\r\nexisting tests\r\n", "huaxingao", "spark", "aggExplain"], ["33896", "2021-09-02T05:21:50Z", "2021-11-10T03:34:32Z", "[SPARK-33701][SHUFFLE] Adaptive shuffle merge finalization for push-based shuffle", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAs part of SPARK-32920 implemented a simple approach to finalization for push-based shuffle. Shuffle merge finalization is the final operation happens at the end of the stage when all the tasks are completed asking all the external shuffle services to complete the shuffle merge for the stage. Once this request is completed no more shuffle pushes will be accepted. With this approach, `DAGScheduler` waits for a fixed time of 10s (`spark.shuffle.push.finalize.timeout`) to allow some time for the inflight shuffle pushes to complete, but this adds additional overhead to stages with very little shuffles.\r\n\r\nIn this PR, instead of waiting for fixed amount of time before shuffle merge finalization now this is controlled adaptively if min threshold number of map tasks shuffle push (`spark.shuffle.push.minPushRatio`) completed then shuffle merge finalization will be scheduled. Also additionally if the total shuffle generated is lesser than min threshold shuffle size (`spark.shuffle.push.minShuffleSizeToWait`) then immediately shuffle merge finalization is scheduled.\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThis is a performance improvement to the existing functionality\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes additional user facing configs `spark.shuffle.push.minPushRatio` and `spark.shuffle.push.minShuffleSizeToWait`\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit tests in `DAGSchedulerSuite`\r\n\r\nLead-authored-by: Min Shen <mshen@linkedin.com>\r\nCo-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>", "venkata91", "spark", "SPARK-33701"], ["34497", "2021-11-05T22:16:41Z", "2021-11-10T03:02:09Z", "[SPARK-37219][SQL] Add AS OF syntax support", "### What changes were proposed in this pull request?\r\nhttps://docs.databricks.com/delta/quick-start.html#query-an-earlier-version-of-the-table-time-travel\r\n\r\nDelta Lake time travel allows user to query an older snapshot of a Delta table. To query an older version of a table, user needs to specify a version or timestamp in a SELECT statement using AS OF syntax as the follows\r\n\r\n```\r\nSELECT * FROM default.people10m VERSION AS OF 0;\r\n\r\nSELECT * FROM default.people10m TIMESTAMP AS OF '2019-01-29 00:37:58';\r\n```\r\n\r\nThis PR adds the AS OF syntax support in Spark\r\n\r\n\r\n### Why are the changes needed?\r\nTo support time travel in Spark\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes\r\n\r\nnew SQL syntax:\r\n```\r\nSELECT * FROM default.people10m VERSION AS OF 0;\r\n\r\nSELECT * FROM default.people10m TIMESTAMP AS OF '2019-01-29 00:37:58';\r\n```\r\n\r\nnew `TableCatalog` property to set options to query time travel table:\r\n\r\n```\r\n  /**\r\n   * A reserved property to specify the version of the table.\r\n   */\r\n  String PROP_VERSION = \"versionAsOf\";\r\n\r\n  /**\r\n   * A reserved property to specify the timestamp of the table.\r\n   */\r\n  String PROP_TIMESTAMP = \"timestampAsOf\";\r\n\r\n\r\ndf1 = spark.read.format('delta').option(TableCatalog.PROP_TIMESTAMP, '2019-01-01').load('/mnt/delta/people-10m')\r\n\r\ndf2 = spark.read.format('delta').option('TableCatalog.PROP_VERSION, 2).load('/mnt/delta/people-10m')\r\n```\r\n\r\n### How was this patch tested?\r\nnew UT\r\n", "huaxingao", "spark", "asof"], ["34489", "2021-11-05T07:47:05Z", "2021-11-10T02:54:08Z", "[SPARK-37210][SQL] Write to static partition in dynamic write mode", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWhen using static partition writing, dynamicPartitionOverwrite should also be set to true. See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for details\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nAn error occurred while concurrently writing to different static partitions.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nSee [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for specific test.", "wForget", "spark", "SPARK-37210"], ["34532", "2021-11-09T12:40:41Z", "2021-11-10T02:34:11Z", "[SPARK-37256][SQL] Replace `ScalaObjectMapper` with `ClassTagExtensions` to fix compilation warning", "### What changes were proposed in this pull request?\r\nThere are some compilation warning log like follows:\r\n```\r\n[WARNING] [Warn] /spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala:268: [deprecation @ org.apache.spark.sql.catalyst.util.RebaseDateTime.loadRebaseRecords.mapper.$anon | origin=com.fasterxml.jackson.module.scala.ScalaObjectMapper | version=2.12.1] trait ScalaObjectMapper in package scala is deprecated (since 2.12.1): ScalaObjectMapper is deprecated because Manifests are not supported in Scala3 \r\n```\r\n\r\nRefer to the recommendations of `jackson-module-scala`, this PR use `ClassTagExtensions`  instead of `ScalaObjectMapper`  to fix this compilation warning\r\n\r\n\r\n### Why are the changes needed?\r\nFix compilation warning\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nPass the Jenkins or GitHub Action\r\n\r\n", "LuciferYang", "spark", "fix-ScalaObjectMapper"], ["34539", "2021-11-09T23:11:14Z", "2021-11-10T02:02:10Z", "[SPARK-37120][BUILD][FOLLOWUP] Test master branch and skip mima/unidoc in Java11/17 tests", "### What changes were proposed in this pull request?\r\n\r\nThis is a follow-up of #34508 . \r\nThis PR aims to switch the test branch from `branch-3.2` to `master` and skip MiMa/unidoc in Java11/17 tests.\r\n\r\n### Why are the changes needed?\r\n\r\nApache Spark use only Java 8 for MiMa/Unidoc steps.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nN/A", "dongjoon-hyun", "spark", "SPARK-37120-3"], ["34515", "2021-11-08T07:12:24Z", "2021-11-10T01:59:26Z", "[SPARK-37235][PYTHON] Inline type hints for python/pyspark/mllib/stat/distribution.py and __init__.py", "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Inline type hints for python/pyspark/mllib/stat/distribution.py and __init__.py\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->We can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->No\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->Existing Tests\r\n", "dchvn", "spark", "SPARK-37235"], ["34534", "2021-11-09T13:12:13Z", "2021-11-10T01:20:42Z", "[MINOR][CORE] Fix error message when requested executor memory exceeds the worker memory", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nFix a typo in the error message.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nTo correct the error message.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, users would see the correct error message.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nPass existing tests.\r\n", "Ngone51", "spark", "fix-error-msg"], ["34492", "2021-11-05T10:19:38Z", "2021-11-10T01:08:19Z", "[SPARK-37216][SQL] Add the Hive macro functionality to SparkSQL", "\r\n### What changes were proposed in this pull request?\r\n\r\nAdd the Hive macro functionality to SparkSQL\r\n\r\n### Why are the changes needed?\r\n\r\nSome Hive sql can move to SparkSQL Smoothly\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nSome new DDL like 'create temparory macro ...'\r\n\r\n### How was this patch tested?\r\n\r\nunit test\r\n\r\nAuthored-by: hgs19921112 <haoguangshi@gmail.com>", "hgs19921112", "spark", "master"], ["32655", "2021-05-24T22:46:36Z", "2021-11-10T00:37:20Z", "[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\nSPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\noverride def getJDBCType(dt: DataType): Option[JdbcType] = dt match {\r\ncase TimestampType => Some(JdbcType(\"DATETIME2\", java.sql.Types.TIMESTAMP))\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nSpark datetime type is timestamp type. This supports a microsecond resolution.\r\nSql supports 2 date time types:\r\n\r\ndatetime can support only milli seconds resolution (0 to 999).\r\ndatetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.\r\ndatetime2 (Transact-SQL) - SQL Server | Microsoft Docs\r\ndatetime (Transact-SQL) - SQL Server | Microsoft Docs\r\n\r\nCurrently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit tests were updated and passed in JDBCSuit.scala.\r\nE2E test done with SQL Server.", "luxu1-ms", "spark", "change-datetime-match"], ["34464", "2021-11-02T03:40:42Z", "2021-11-10T00:09:46Z", "[SPARK-37193][SQL] DynamicJoinSelection.shouldDemoteBroadcastHashJoin should not apply to outer joins", "\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nDynamicJoinSelection.shouldDemoteBroadcastHashJoin will prevent AQE from converting Sort merge join into a broadcast join because SMJ is faster when the side that would be broadcast has a lot of empty partitions.\r\nThis makes sense for inner joins which can short circuit if one side is empty.\r\nFor (left,right) outer join, the streaming side still has to be processed so demoting broadcast join doesn't have the same advantage.\r\n\r\n\r\n\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, it may cause AQE to choose BHJ more often than before with better performance\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nSpark UTs\r\nAlso empirical evidence", "ekoifman", "spark", "SPARK-37193"], ["34505", "2021-11-07T05:20:22Z", "2021-11-10T00:08:01Z", "[SPARK-37228][SQL][PYTHON] Implement DataFrame.mapInArrow in Python", "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to implement `DataFrame.mapInArrow` that allows users to apply a function with PyArrow record batches such as:\r\n\r\n```python\r\ndef do_something(iterator):\r\n    for arrow_batch in iterator:\r\n        # do something with `pyarrow.RecordBatch` and create new `pyarrow.RecordBatch`.\r\n        # ...\r\n        yield arrow_batch\r\n\r\ndf.mapInArrow(do_something, df.schema).show()\r\n```\r\n\r\nThe general idea is simple. It shares the same codebase of `DataFrame.mapInPandas` except the pandas conversion logic.\r\n\r\nNote that documentation will be done in another PR.\r\n\r\n### Why are the changes needed?\r\n\r\nFor usability and technical problems. Both are elabourated in more details at SPARK-37227.\r\nPlease also see the discussions at https://github.com/apache/spark/pull/26783.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, this PR adds a new API:\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\ndf = spark.createDataFrame(\r\n    [(1, \"foo\"), (2, None), (3, \"bar\"), (4, \"bar\")], \"a int, b string\")\r\n\r\ndef func(iterator):\r\n    for batch in iterator:\r\n        # `batch` is pyarrow.RecordBatch.\r\n        yield batch\r\n\r\ndf.mapInArrow(func, df.schema).collect()\r\n```\r\n\r\n### How was this patch tested?\r\n\r\nManually tested, and unit tests were added.", "HyukjinKwon", "spark", "SPARK-37228"], ["34273", "2021-10-13T10:53:46Z", "2021-11-10T00:02:10Z", "[SPARK-36997][PYTHON][TESTS] Run mypy tests against ml, sql, streaming and core examples", "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR:\r\n\r\n- Adds `mypy_examples_test` and `mypy_annotation_test` and refactors `mypy_test` in `dev/lint-python` to enable testing PySpark examples with mypy.\r\n- Adjusts examples for `ml`, `sql`, `streaming` and core to address detected type checking issues.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe goal of this PR is to improve test coverage of type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nIn general, no. \r\n\r\nThe only change, directly visible to the end user, are small adjustments to the example scripts.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExisting tests with additions listed above.\r\n", "zero323", "spark", "SPARK-36997"]]