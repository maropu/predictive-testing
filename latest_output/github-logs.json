{"author": "linhongliu-db", "sha": "", "commit_date": "2021/09/12 17:07:27", "commit_message": "[SPARK-36729][BUILD] Upgrade Netty from 4.1.63 to 4.1.68\n\n### What changes were proposed in this pull request?\n\nThis PR upgrades Netty from `4.1.63` to `4.1.68`.\n\nAll the changes from `4.1.64` to `4.1.68` are as follows.\n\n* 4.1.64 and 4.1.65\n  * https://netty.io/news/2021/05/19/4-1-65-Final.html\n* 4.1.66\n  * https://netty.io/news/2021/07/16/4-1-66-Final.html\n* 4.1.67\n  * https://netty.io/news/2021/08/16/4-1-67-Final.html\n* 4.1.68\n  * https://netty.io/news/2021/09/09/4-1-68-Final.html\n\n### Why are the changes needed?\n\nRecently Netty `4.1.68` was released, which includes official M1 Mac support.\n* Add support for mac m1\n  * https://github.com/netty/netty/pull/11666\n\n`4.1.65` also includes a critical bug fix which Spark might be affected.\n* JNI classloader deadlock with latest JDK version\n  * https://github.com/netty/netty/issues/11209\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nCIs.\n\nCloses #33970 from sarutak/upgrade-netty-4.1.68.\n\nAuthored-by: Kousuke Saruta <sarutak@oss.nttdata.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 14]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 14]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 23]}]}
{"author": "sarutak", "sha": "", "commit_date": "2021/11/06 00:32:32", "commit_message": "[SPARK-36998][CORE] Handle concurrent eviction of same application in SHS\n\n### What changes were proposed in this pull request?\n To gracefully handle the error thrown when we try to make room for parsing of different applications and they try to evict the same application by deleting the directory path.\n\nAlso, added a test for `deleteStore` in `HistoryServerDiskManagerSuite`\n\n ### Why are the changes needed?\n Otherwise, an NoSuchFileException is thrown when it cannot find the directory path to exist.\n\n ### Does this PR introduce _any_ user-facing change?\n no\n\n ### How was this patch tested?\nAdded a unit test for `deleteStore` but not specifically testing the concurrency fix.\n\nCloses #34276 from thejdeep/SPARK-36998.\n\nAuthored-by: Thejdeep Gudivada <tgudivada@linkedin.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerDiskManagerSuite.scala", "additions": "27", "deletions": "3", "changes": "30"}, "updated": [1, 1, 1]}]}
{"author": "gengliangwang", "sha": "", "commit_date": "2021/09/27 07:38:39", "commit_message": "hadoop-provided", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 2, 7]}]}
{"author": "AngersZhuuuu", "sha": "", "commit_date": "2021/10/28 02:38:01", "commit_message": "[SPARK-37036][PYTHON] Add util function to raise advice warning for pandas API on Spark\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to add an util function to raise advice warning for pandas API on Spark.\n\nApart from the existing warnings recognized by general Python, PySpark and pandas users, these warnings are things to pay special attention to in the pandas API on Spark, so I think it is better to manage warnings separately.\n\nThis PR basically do:\n- Add util function, `log_advice` to raise `UserWarning` with proper message.\n- Add `log_advice` to the APIs where needs showing the advice.\n\n### Why are the changes needed?\n\nThe pandas API on Spark has functions that the existing pandas users who are not familiar with distributed environment should aware for avoiding confusion (not only confusion, it also could cause the serious performance degradation).\n\nFor example:\n- `sort_index`, `len`, `sort_values`: such functions can cause the performance degradation since it goes through the entire data set\n- `to_xxx`, `read_xxx`: if the `index_col` is not specified for some I/O functions, the default index is attached which is expensive (and also the existing index will be lost)\n- `to_list`, `to_pandas`, `to_markdown`: such functions load the whole data into the driver's memory, so potentially could cause the OOM.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the pandas-on-Spark users can see the warning when they're using inefficient or potentially dangerous functions.\n\n### How was this patch tested?\n\nManually check the behavior one-by-one.\n\n```python\n>>> import pyspark.pandas as ps\n>>> psser = ps.Series([1, 2, 3, 4])\n>>> psser.to_list()\n.../spark/python/pyspark/pandas/utils.py:968: PandasAPIOnSparkAdviceWarning: `to_list` loads the all data into the driver's memory. It should only be used if the resulting list is expected to be small.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n[1, 2, 3, 4]\n```\n\nCloses #34389 from itholic/SPARK-37036.\n\nLead-authored-by: itholic <haejoon.lee@databricks.com>\nCo-authored-by: Haejoon Lee <44108233+itholic@users.noreply.github.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/accessors.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 2, 7]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [1, 4, 15]}, {"file": {"name": "python/pyspark/pandas/generic.py", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 3, 5]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 2, 9]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 1, 7]}, {"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 3, 9]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 3, 11]}, {"file": {"name": "python/pyspark/pandas/utils.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 2, 4]}]}
{"author": "Ngone51", "sha": "ce22c0968fe86dbbdee31bceb233071c43d02c8f", "commit_date": "2021/11/09 14:50:42", "commit_message": "fix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "sleep1661", "sha": "", "commit_date": "2021/10/09 01:19:36", "commit_message": "[SPARK-36944][PYTHON] Remove python/pyspark/sql/__init__.pyi\n\n### What changes were proposed in this pull request?\nRemove python/pyspark/sql/\\_\\_init\\_\\_.pyi\n\n### Why are the changes needed?\nRemove python/pyspark/sql/\\_\\_init\\_\\_.pyi\n\n### Does this PR introduce _any_ user-facing change?\nNo\n### How was this patch tested?\nexisting tests\n\nCloses #34203 from dchvn/remove_init_.pyi.\n\nAuthored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/__init__.pyi", "additions": "0", "deletions": "42", "changes": "42"}, "updated": [1, 1, 1]}]}
{"author": "MaxGekk", "sha": "", "commit_date": "2021/11/09 16:07:38", "commit_message": "[SPARK-37239][YARN][TESTS][FOLLOWUP] Add UT to cover `Client.prepareLocalResources` with custom `STAGING_FILE_REPLICATION`\n\n### What changes were proposed in this pull request?\nThis pr add a new UT to cover `o.a.s.deploy.yarn.Client.prepareLocalResources` method with custom `STAGING_FILE_REPLICATION` configuration and change other related UTs to verify that the `replication` passed into the `copyFileToRemote` method is `None` explicitly.\n\n### Why are the changes needed?\nAdd new UT.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPass the Jenkins or GitHub Action\n\nCloses #34531 from LuciferYang/SPARK-37239-followup.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala", "additions": "27", "deletions": "8", "changes": "35"}, "updated": [2, 3, 4]}]}
{"author": "dchvn", "sha": "", "commit_date": "2021/11/09 23:40:17", "commit_message": "[MINOR][CORE] Fix error message when requested executor memory exceeds the worker memory\n\n### What changes were proposed in this pull request?\n\nFix a typo in the error message.\n\n### Why are the changes needed?\n\nTo correct the error message.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, users would see the correct error message.\n\n### How was this patch tested?\n\nPass existing tests.\n\nCloses #34534 from Ngone51/fix-error-msg.\n\nAuthored-by: yi.wu <yi.wu@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}]}
{"author": "huaxingao", "sha": "", "commit_date": "2021/11/04 14:26:34", "commit_message": "[SPARK-37038][SQL] DSV2 Sample Push Down\n\n### What changes were proposed in this pull request?\n\nPush down Sample to data source for better performance. If Sample is pushed down, it will be removed from logical plan so it will not be applied at Spark any more.\n\nCurrent Plan without Sample push down:\n```\n== Parsed Logical Plan ==\n'Project [*]\n+- 'Sample 0.0, 0.8, false, 157\n   +- 'UnresolvedRelation [postgresql, new_table], [], false\n\n== Analyzed Logical Plan ==\ncol1: int, col2: int\nProject [col1#163, col2#164]\n+- Sample 0.0, 0.8, false, 157\n   +- SubqueryAlias postgresql.new_table\n      +- RelationV2[col1#163, col2#164] new_table\n\n== Optimized Logical Plan ==\nSample 0.0, 0.8, false, 157\n+- RelationV2[col1#163, col2#164] new_table\n\n== Physical Plan ==\n*(1) Sample 0.0, 0.8, false, 157\n+- *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$16dde4769 [col1#163,col2#164] PushedAggregates: [], PushedFilters: [], PushedGroupby: [],  ReadSchema: struct<col1:int,col2:int>\n```\nafter Sample push down:\n```\n== Parsed Logical Plan ==\n'Project [*]\n+- 'Sample 0.0, 0.8, false, 187\n   +- 'UnresolvedRelation [postgresql, new_table], [], false\n\n== Analyzed Logical Plan ==\ncol1: int, col2: int\nProject [col1#163, col2#164]\n+- Sample 0.0, 0.8, false, 187\n   +- SubqueryAlias postgresql.new_table\n      +- RelationV2[col1#163, col2#164] new_table\n\n== Optimized Logical Plan ==\nRelationV2[col1#163, col2#164] new_table\n\n== Physical Plan ==\n*(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$165b57543 [col1#163,col2#164] PushedAggregates: [], PushedFilters: [], PushedGroupby: [], PushedSample: TABLESAMPLE  0.0 0.8 false 187, ReadSchema: struct<col1:int,col2:int>\n```\nThe new interface is implemented using JDBC for POC and end to end test. TABLESAMPLE is not supported by all the databases. It is implemented using postgresql in this PR.\n\n### Why are the changes needed?\nReduce IO and improve performance. For SAMPLE, e.g. `SELECT * FROM t TABLESAMPLE (1 PERCENT)`, Spark retrieves all the data from table and then return 1% rows. It will dramatically reduce the transferred data size and improve performance if we can push Sample to data source side.\n\n### Does this PR introduce any user-facing change?\nYes. new interface `SupportsPushDownTableSample`\n\n### How was this patch tested?\nNew test\n\nCloses #34451 from huaxingao/sample.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-jdbc.md", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [2, 3, 3]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/PostgresIntegrationSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 2, 2]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "110", "deletions": "0", "changes": "110"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/ScanBuilder.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownTableSample.java", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "4", "deletions": "6", "changes": "10"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [2, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [3, 5, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushedDownOperators.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/TableSampleInfo.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "32", "deletions": "9", "changes": "41"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala", "additions": "27", "deletions": "10", "changes": "37"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "16", "deletions": "3", "changes": "19"}, "updated": [1, 2, 2]}]}
{"author": "LuciferYang", "sha": "", "commit_date": "2021/10/27 07:32:47", "commit_message": "[SPARK-37115][SQL] HiveClientImpl should use shim to wrap all hive client calls\n\n### What changes were proposed in this pull request?\nIn this pr we use `shim` to wrap all hive client api to make it easier.\n\n### Why are the changes needed?\nUse `shim` to wrap all hive client api  to make it easier.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisted UT\n\nCloses #34388 from AngersZhuuuu/SPARK-37115.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [3, 4, 5]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "173", "deletions": "3", "changes": "176"}, "updated": [3, 3, 3]}]}
{"author": "dongjoon-hyun", "sha": "", "commit_date": "2021/11/05 05:38:59", "commit_message": "[SPARK-37137][PYTHON] Inline type hints for python/pyspark/conf.py\n\n### What changes were proposed in this pull request?\n\nInline type hints for python/pyspark/conf.py\n\n### Why are the changes needed?\n\nCurrently, Inline type hints for python/pyspark/conf.pyi doesn't support type checking within function bodies. So we inline type hints to support that.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExising test.\n\nCloses #34411 from ByronHsu/SPARK-37137.\n\nAuthored-by: Byron <byronhsu1230@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/conf.py", "additions": "42", "deletions": "23", "changes": "65"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/conf.pyi", "additions": "0", "deletions": "44", "changes": "44"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [4, 4, 7]}]}
{"author": "hgs19921112", "sha": "d06f62b0de377f2cf91051d0551166c61c8333c0", "commit_date": "2021/11/05 08:46:58", "commit_message": "add macro", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Macro.scala", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "72", "deletions": "4", "changes": "76"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/UDFSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 1]}]}
{"author": "ekoifman", "sha": "8a3f27d5d1384bf9b7ed768fe5fd65bd9adf84e6", "commit_date": "2021/11/02 03:34:50", "commit_message": "[SPARK-37193][SQL] DynamicJoinSelection.shouldDemoteBroadcastHashJoin should not apply to outer joins", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/DynamicJoinSelection.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}]}
{"author": "HyukjinKwon", "sha": "", "commit_date": "2021/11/09 23:40:17", "commit_message": "[MINOR][CORE] Fix error message when requested executor memory exceeds the worker memory\n\n### What changes were proposed in this pull request?\n\nFix a typo in the error message.\n\n### Why are the changes needed?\n\nTo correct the error message.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, users would see the correct error message.\n\n### How was this patch tested?\n\nPass existing tests.\n\nCloses #34534 from Ngone51/fix-error-msg.\n\nAuthored-by: yi.wu <yi.wu@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}]}
{"author": "zero323", "sha": "", "commit_date": "2021/10/24 02:48:44", "commit_message": "Revert \"[SPARK-37103][INFRA] Switch from Maven to SBT to build Spark on AppVeyor\"\n\nThis reverts commit 3fc16fc7909937ebfa0bb8382ba20c7af641fba1.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "appveyor.yml", "additions": "2", "deletions": "6", "changes": "8"}, "updated": [2, 2, 2]}, {"file": {"name": "dev/appveyor-install-dependencies.ps1", "additions": "6", "deletions": "20", "changes": "26"}, "updated": [2, 2, 2]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [3, 8, 11]}, {"file": {"name": "project/build.properties", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [2, 2, 2]}]}
